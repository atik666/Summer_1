{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T11:21:34.180980Z",
     "start_time": "2020-04-16T11:21:34.176870Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "root_path = './../datasets'\n",
    "# processed_folder =  os.path.join(root_path)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(os.path.join(root_path,'mini-imagenet.zip'), 'r')\n",
    "# zip_ref.extractall(root_path)\n",
    "# zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T11:21:34.808635Z",
     "start_time": "2020-04-16T11:21:34.771264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import collections\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "class MiniImagenet(Dataset):\n",
    "    \"\"\"\n",
    "    put mini-imagenet files as :\n",
    "    root :\n",
    "        |- images/*.jpg includes all images\n",
    "        |- train.csv\n",
    "        |- test.csv\n",
    "        |- val.csv\n",
    "    NOTICE: meta-learning is different from general supervised learning, especially the concept of batch and set.\n",
    "    batch: contains several sets\n",
    "    sets: conains n_way * k_shot for meta-train set, n_way * n_query for meta-test set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, mode, batchsz, n_way, k_shot, k_query, resize, startidx=0):\n",
    "        \"\"\"\n",
    "        :param startidx: start to index label from startidx\n",
    "        \"\"\"\n",
    "\n",
    "        self.batchsz = batchsz  # batch of set, not batch of imgs\n",
    "        self.n_way = n_way  # n-way\n",
    "        self.k_shot = k_shot  # k-shot\n",
    "        self.k_query = k_query  # for evaluation\n",
    "        self.setsz = self.n_way * self.k_shot  # num of samples per set\n",
    "        self.querysz = self.n_way * self.k_query  # number of samples per set for evaluation\n",
    "        self.resize = resize  # resize to\n",
    "        self.startidx = startidx  # index label not from 0, but from startidx\n",
    "        print('shuffle DB :%s, b:%d, %d-way, %d-shot, %d-query, resize:%d' % (mode, batchsz, n_way, k_shot, k_query, resize))\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.transform = transforms.Compose([lambda x: Image.open(x).convert('RGB'),\n",
    "                                                 transforms.Resize((self.resize, self.resize)),\n",
    "                                                 # transforms.RandomHorizontalFlip(),\n",
    "                                                 # transforms.RandomRotation(5),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                                 ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([lambda x: Image.open(x).convert('RGB'),\n",
    "                                                 transforms.Resize((self.resize, self.resize)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                                 ])\n",
    "\n",
    "        self.path = os.path.join(root, 'images')  # image path\n",
    "        \n",
    "        # :return: dictLabels: {label1: [filename1, filename2, filename3, filename4,...], }\n",
    "        dictLabels = self.loadCSV(os.path.join(root, mode + '.csv'))  # csv path\n",
    "        self.data = []\n",
    "        self.img2label = {}\n",
    "        for i, (label, imgs) in enumerate(dictLabels.items()):\n",
    "            self.data.append(imgs)  # [[img1, img2, ...], [img111, ...]]\n",
    "            self.img2label[label] = i + self.startidx  # {\"img_name[:9]\":label}\n",
    "        self.cls_num = len(self.data)\n",
    "\n",
    "        self.create_batch(self.batchsz)\n",
    "\n",
    "    def loadCSV(self, csvf):\n",
    "        \"\"\"\n",
    "        return a dict saving the information of csv\n",
    "        :param splitFile: csv file name\n",
    "        :return: {label:[file1, file2 ...]}\n",
    "        \"\"\"\n",
    "        dictLabels = {}\n",
    "        with open(csvf) as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            next(csvreader, None)  # skip (filename, label)\n",
    "            for i, row in enumerate(csvreader):\n",
    "                filename = row[0]\n",
    "                label = row[1]\n",
    "                # append filename to current label\n",
    "                if label in dictLabels.keys():\n",
    "                    dictLabels[label].append(filename)\n",
    "                else:\n",
    "                    dictLabels[label] = [filename]\n",
    "        return dictLabels\n",
    "\n",
    "    def create_batch(self, batchsz):\n",
    "        \"\"\"\n",
    "        create batch for meta-learning.\n",
    "        ×episode× here means batch, and it means how many sets we want to retain.\n",
    "        :param episodes: batch size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.support_x_batch = []  # support set batch\n",
    "        self.query_x_batch = []  # query set batch\n",
    "        for b in range(batchsz):  # for each batch\n",
    "            # 1.select n_way classes randomly\n",
    "            selected_cls = np.random.choice(self.cls_num, self.n_way, False)  # no duplicate\n",
    "            np.random.shuffle(selected_cls)\n",
    "            support_x = []\n",
    "            query_x = []\n",
    "            for cls in selected_cls:\n",
    "                # 2. select k_shot + k_query for each class\n",
    "                selected_imgs_idx = np.random.choice(len(self.data[cls]), self.k_shot + self.k_query, False)\n",
    "                np.random.shuffle(selected_imgs_idx)\n",
    "                indexDtrain = np.array(selected_imgs_idx[:self.k_shot])  # idx for Dtrain\n",
    "                indexDtest = np.array(selected_imgs_idx[self.k_shot:])  # idx for Dtest\n",
    "                support_x.append(\n",
    "                    np.array(self.data[cls])[indexDtrain].tolist())  # get all images filename for current Dtrain\n",
    "                query_x.append(np.array(self.data[cls])[indexDtest].tolist())\n",
    "\n",
    "            # shuffle the correponding relation between support set and query set\n",
    "            random.shuffle(support_x)\n",
    "            random.shuffle(query_x)\n",
    "\n",
    "            self.support_x_batch.append(support_x)  # append set to current sets\n",
    "            self.query_x_batch.append(query_x)  # append sets to current sets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        index means index of sets, 0<= index <= batchsz-1\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # [setsz, 3, resize, resize]\n",
    "        support_x = torch.FloatTensor(self.setsz, 3, self.resize, self.resize)\n",
    "        # [setsz]\n",
    "        support_y = np.zeros((self.setsz), dtype=np.int)\n",
    "        # [querysz, 3, resize, resize]\n",
    "        query_x = torch.FloatTensor(self.querysz, 3, self.resize, self.resize)\n",
    "        # [querysz]\n",
    "        query_y = np.zeros((self.querysz), dtype=np.int)\n",
    "\n",
    "        flatten_support_x = [os.path.join(self.path, item)\n",
    "                             for sublist in self.support_x_batch[index] for item in sublist]\n",
    "        support_y = np.array(\n",
    "            [self.img2label[item[:9]]  # filename:n0153282900000005.jpg, the first 9 characters treated as label\n",
    "             for sublist in self.support_x_batch[index] for item in sublist]).astype(np.int32)\n",
    "\n",
    "        flatten_query_x = [os.path.join(self.path, item)\n",
    "                           for sublist in self.query_x_batch[index] for item in sublist]\n",
    "        query_y = np.array([self.img2label[item[:9]]\n",
    "                            for sublist in self.query_x_batch[index] for item in sublist]).astype(np.int32)\n",
    "\n",
    "        # print('global:', support_y, query_y)\n",
    "        # support_y: [setsz]\n",
    "        # query_y: [querysz]\n",
    "        # unique: [n-way], sorted\n",
    "        unique = np.unique(support_y)\n",
    "        random.shuffle(unique)\n",
    "        # relative means the label ranges from 0 to n-way\n",
    "        support_y_relative = np.zeros(self.setsz)\n",
    "        query_y_relative = np.zeros(self.querysz)\n",
    "        for idx, l in enumerate(unique):\n",
    "            support_y_relative[support_y == l] = idx\n",
    "            query_y_relative[query_y == l] = idx\n",
    "\n",
    "        # print('relative:', support_y_relative, query_y_relative)\n",
    "\n",
    "        for i, path in enumerate(flatten_support_x):\n",
    "            support_x[i] = self.transform(path)\n",
    "\n",
    "        for i, path in enumerate(flatten_query_x):\n",
    "            query_x[i] = self.transform(path)\n",
    "\n",
    "        return support_x, torch.LongTensor(support_y_relative), query_x, torch.LongTensor(query_y_relative)\n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.batchsz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T11:21:34.931470Z",
     "start_time": "2020-04-16T11:21:34.927568Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spt_x_batch = []\n",
    "# qry_x_batch = []\n",
    "\n",
    "# dtrain = MiniImagenet('./../datasets/mini-imagenet/', mode='train', n_way=5, k_shot=1,\n",
    "#                     k_query=15,\n",
    "#                     batchsz=10000, resize=84)\n",
    "\n",
    "\n",
    "# for b in range(10000):\n",
    "#     selected_cls = np.random.choice(65, 5, False)\n",
    "#     np.random.shuffle(selected_cls)\n",
    "    \n",
    "#     ## 构造支持集和查询集x\n",
    "#     spt_x = []\n",
    "#     qry_x = []\n",
    "#     for cls in selected_cls:\n",
    "#         selected_imgs_idx = np.random.choice(len(dtrain.data[cls]), 1 + 15, False)\n",
    "#         np.random.shuffle(selected_imgs_idx)\n",
    "#         indexDtrain = np.array(selected_imgs_idx[:1])\n",
    "#         indexDtest = np.array(selected_imgs_idx[1:])\n",
    "#         spt_x.append(np.array(dtrain.data[cls])[indexDtrain].tolist())\n",
    "#         qry_x.append(np.array(Dtrain.data[cls])[indexDtest].tolist())\n",
    "#     random.shuffle(spt_x) \n",
    "#     random.shuffle(qry_x)\n",
    "    \n",
    "#     spt_x_batch.append(spt_x)\n",
    "#     qry_x_batch.append(qry_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T11:21:35.136013Z",
     "start_time": "2020-04-16T11:21:35.130778Z"
    }
   },
   "outputs": [],
   "source": [
    "class Hello():\n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "    def __getitem__(self,key):\n",
    "        print('hello')\n",
    "ins = Hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T11:21:35.788489Z",
     "start_time": "2020-04-16T11:21:35.756783Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    定义一个网络\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(Learner, self).__init__()\n",
    "        self.config = config ## 对模型各个超参数的定义\n",
    "        '''\n",
    "        ## ParameterList可以像普通Python列表一样进行索引，\n",
    "        但是它包含的参数已经被正确注册，并且将被所有的Module方法都可见。\n",
    "        \n",
    "        '''\n",
    "        self.vars = nn.ParameterList() ## 这个字典中包含了所有需要被优化的tensor\n",
    "        self.vars_bn = nn.ParameterList()  \n",
    "        \n",
    "        for i, (name, param) in enumerate(self.config):\n",
    "            if name is 'conv2d':\n",
    "                ## [ch_out, ch_in, kernel_size, kernel_size]\n",
    "                weight = nn.Parameter(torch.ones(*param[:4])) ## 产生*param大小的全为1的tensor\n",
    "                torch.nn.init.kaiming_normal_(weight) ## 初始化权重\n",
    "                self.vars.append(weight) ## 加到nn.ParameterList中\n",
    "                \n",
    "                bias = nn.Parameter(torch.zeros(param[0]))\n",
    "                self.vars.append(bias)\n",
    "                \n",
    "            elif name is 'linear':\n",
    "                weight = nn.Parameter(torch.ones(*param))\n",
    "                torch.nn.init.kaiming_normal_(weight)\n",
    "                self.vars.append(weight)\n",
    "                bias  = nn.Parameter(torch.zeros(param[0]))\n",
    "                self.vars.append(bias)\n",
    "            \n",
    "            elif name is 'bn':\n",
    "                ## 对小批量(mini-batch)的2d或3d输入进行批标准化(Batch Normalization)操作,\n",
    "                ## BN层在训练过程中，会将一个Batch的中的数据转变成正态分布\n",
    "                weight = nn.Parameter(torch.ones(param[0]))\n",
    "                self.vars.append(weight)\n",
    "                bias = nn.Parameter(torch.zeros(param[0]))\n",
    "                self.vars.append(bias)\n",
    "                \n",
    "                ### \n",
    "                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad = False)\n",
    "                running_var = nn.Parameter(torch.zeros(param[0]), requires_grad = False)\n",
    "                \n",
    "                self.vars_bn.extend([running_mean, running_var]) ## 在列表附加参数\n",
    "                \n",
    "            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n",
    "                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                raise NotImplementedError       \n",
    "    \n",
    "    \n",
    "    ## self.net(x_support[i], vars=None, bn_training = True)\n",
    "    ## x: torch.Size([5, 1, 28, 28])\n",
    "    ## 构造模型\n",
    "    def forward(self, x, vars = None, bn_training=True):\n",
    "        '''\n",
    "        :param bn_training: set False to not update\n",
    "        :return: \n",
    "        '''\n",
    "        \n",
    "        if vars is None:\n",
    "            vars = self.vars\n",
    "            \n",
    "        idx = 0 ; bn_idx = 0\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                weight, bias = vars[idx], vars[idx + 1]\n",
    "                x = F.conv2d(x, weight, bias, stride = param[4], padding = param[5]) \n",
    "                idx += 2\n",
    "                \n",
    "            elif name is 'linear':\n",
    "                weight, bias = vars[idx], vars[idx + 1]\n",
    "                x = F.linear(x, weight, bias)\n",
    "                idx += 2\n",
    "                \n",
    "            elif name is 'bn':\n",
    "                weight, bias = vars[idx], vars[idx + 1]\n",
    "                running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx + 1]\n",
    "                x = F.batch_norm(x, running_mean, running_var, weight= weight, bias = bias, training = bn_training)\n",
    "                idx += 2\n",
    "                bn_idx += 2\n",
    "            \n",
    "            elif name is 'flatten':\n",
    "                x = x.view(x.size(0), -1)\n",
    "            \n",
    "            elif name is 'relu':\n",
    "                x = F.relu(x, inplace = [param[0]])\n",
    "            \n",
    "            elif name is 'reshape':\n",
    "                # [b, 8] => [b, 2, 2, 2]\n",
    "                x = x.view(x.size(0), *param)\n",
    "            elif name is 'leakyrelu':\n",
    "                x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n",
    "            elif name is 'tanh':\n",
    "                x = F.tanh(x)\n",
    "            elif name is 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif name is 'upsample':\n",
    "                x = F.upsample_nearest(x, scale_factor=param[0])\n",
    "            elif name is 'max_pool2d':\n",
    "                x = F.max_pool2d(x, param[0], param[1], param[2])\n",
    "            elif name is 'avg_pool2d':\n",
    "                x = F.avg_pool2d(x, param[0], param[1], param[2])\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "        assert idx == len(vars)\n",
    "        assert bn_idx == len(self.vars_bn)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T11:21:37.167465Z",
     "start_time": "2020-04-16T11:21:37.127367Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "\n",
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta-Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(Meta, self).__init__()   \n",
    "        self.update_lr = 0.1 ## learner中的学习率，即\\alpha\n",
    "        self.meta_lr = 1e-3 ## meta-learner的学习率，即\\beta\n",
    "        self.n_way = 5 ## 5种类型\n",
    "        self.k_shot = 5 ## 一个样本\n",
    "        self.k_query = 15 ## 15个查询样本\n",
    "        self.task_num = 4 ## 每轮抽8个任务进行训练\n",
    "        self.update_step = 5 ## task-level inner update steps\n",
    "        self.update_step_test = 5 ## 用在finetunning这个函数中\n",
    "        \n",
    "        self.net = Learner(config) ## base-learner\n",
    "        self.meta_optim = torch.optim.Adam(self.net.parameters(), lr = self.meta_lr)\n",
    "        \n",
    "    def forward(self, x_support, y_support, x_query, y_query):\n",
    "        \"\"\"\n",
    "        :param x_spt:   torch.Size([8, 5, 1, 28, 28])\n",
    "        :param y_spt:   torch.Size([8, 5])\n",
    "        :param x_qry:   torch.Size([8, 75, 1, 28, 28])\n",
    "        :param y_qry:   torch.Size([8, 75])\n",
    "        :return:\n",
    "        N-way-K-shot\n",
    "        \"\"\"\n",
    "        task_num, ways, shots, h, w = x_support.size()\n",
    "#         print(\"Meta forward\")\n",
    "        querysz = x_query.size(1)## 75 = 15*5\n",
    "        losses_q = [0 for _ in range(self.update_step +1)] ## losses_q[i] is the loss on step i\n",
    "        corrects = [0 for _ in range(self.update_step +1)]\n",
    "        \n",
    "        for i in range(task_num):    \n",
    "            \n",
    "            ## 第0步更新\n",
    "            logits = self.net(x_support[i], vars=None, bn_training = True)## return 一个经过各层计算后的y\n",
    "            ## logits : 5*5的tensor\n",
    "            loss = F.cross_entropy(logits, y_support[i])  ## 计算Loss值\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters()) ##计算梯度。如果输入x，输出是y，则求y关于x的导数（梯度）\n",
    "            tuples = zip(grad, self.net.parameters() ) ##将梯度grad和参数\\theta一一对应起来\n",
    "            ## fast_weights这一步相当于求了一个\\theta - \\alpha*\\nabla(L)\n",
    "            fast_weights = list( map(lambda p: p[1] - self.update_lr * p[0], tuples) )\n",
    "            \n",
    "            ### 在query集上进行测试，计算准确率\n",
    "            ## 这一步使用的是更新前的参数\n",
    "            with torch.no_grad():\n",
    "                logits_q = self.net(x_query[i], self.net.parameters(), bn_training = True) ## logits_q :torch.Size([75, 5])\n",
    "                loss_q = F.cross_entropy(logits_q, y_query[i]) ## y_query : torch.Size([75])\n",
    "                losses_q[0] += loss_q ##将loss存在数组的第一个位置\n",
    "                pred_q = F.softmax(logits_q, dim = 1).argmax(dim=1) ## size = (75)\n",
    "                correct = torch.eq(pred_q, y_query[i]).sum().item()## item()取出tensor中的数字\n",
    "                corrects[0] += correct\n",
    "            \n",
    "            ### 在query集上进行测试，计算准确率\n",
    "            ## 这一步使用的是更新后的参数\n",
    "            with torch.no_grad():\n",
    "                logits_q = self.net(x_query[i], fast_weights, bn_training = True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_query[i])\n",
    "                losses_q[1] += loss_q\n",
    "                pred_q = F.softmax(logits_q, dim = 1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_query[i]).sum().item()\n",
    "                corrects[1] += correct\n",
    "             \n",
    "            \n",
    "            for k in range(1, self.update_step):\n",
    "                logits = self.net(x_support[i], fast_weights, bn_training =True)\n",
    "                loss = F.cross_entropy(logits, y_support[i])\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                tuples = zip(grad,fast_weights)\n",
    "                fast_weights = list(map(lambda p:p[1] - self.update_lr * p[0], tuples))\n",
    "                \n",
    "                if k < self.update_step - 1:\n",
    "                    with torch.no_grad():   \n",
    "                        logits_q = self.net(x_query[i], fast_weights, bn_training = True)\n",
    "                        loss_q = F.cross_entropy(logits_q, y_query[i])\n",
    "                        losses_q[k+1] += loss_q\n",
    "                        \n",
    "                else:\n",
    "                    logits_q = self.net(x_query[i], fast_weights, bn_training = True)\n",
    "                    loss_q = F.cross_entropy(logits_q, y_query[i])\n",
    "                    losses_q[k+1] += loss_q\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim = 1)\n",
    "                    correct = torch.eq(pred_q, y_query[i]).sum().item()\n",
    "                    corrects[k+1] += correct\n",
    "                    \n",
    "        ## 在一组8个任务结束后，求一个平均的loss\n",
    "        loss_q = losses_q[-1] / task_num\n",
    "        self.meta_optim.zero_grad() ## 梯度清零\n",
    "        loss_q.backward() ## 计算梯度\n",
    "        self.meta_optim.step() ## 用设置好的优化方法来迭代模型参数，这一步是meta步迭代\n",
    "        \n",
    "        accs = np.array(corrects) / (querysz * task_num) \n",
    "        \n",
    "        return accs\n",
    "        \n",
    "    \n",
    "    def finetunning(self, x_support, y_support, x_query, y_query):\n",
    "        assert len(x_support.shape) == 4\n",
    "        \n",
    "        querysz = x_query.size(0)\n",
    "        \n",
    "        corrects = [0 for _ in range(self.update_step_test + 1)]\n",
    "        \n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        net = deepcopy(self.net)\n",
    "        \n",
    "        logits = net(x_support)\n",
    "        loss = F.cross_entropy(logits, y_support)\n",
    "        grad = torch.autograd.grad(loss, net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, net.parameters())))\n",
    "        \n",
    "        \n",
    "        ## 开始训练前的准确率\n",
    "        with torch.no_grad():\n",
    "            logits_q = net(x_query, net.parameters(), bn_training = True)\n",
    "            pred_q = F.softmax(logits_q, dim =1).argmax(dim=1)\n",
    "            correct = torch.eq(pred_q, y_query).sum().item()\n",
    "            corrects[0] += correct\n",
    "         \n",
    "        ## 训练后的准确率\n",
    "        with torch.no_grad():\n",
    "            logits_q = net(x_query, fast_weights, bn_training = True)\n",
    "            pred_q = F.softmax(logits_q, dim = 1).argmax(dim=1)\n",
    "            correct = torch.eq(pred_q, y_query).sum().item()\n",
    "            corrects[1] += correct\n",
    "            \n",
    "        for k in range(1, self.update_step_test):\n",
    "            logits = net(x_support, fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_support)\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "            \n",
    "            logits_q = net(x_query, fast_weights, bn_training=True)\n",
    "            loss_q = F.cross_entropy(logits_q, y_query)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred_q = F.softmax(logits_q, dim =1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_query).sum().item()\n",
    "                corrects[k+1] += correct\n",
    "                \n",
    "        del net\n",
    "        \n",
    "        accs = np.array(corrects) / querysz\n",
    "        \n",
    "        return accs\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:32:29.035200Z",
     "start_time": "2020-04-16T11:23:38.287146Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle DB :train, b:10000, 5-way, 5-shot, 15-query, resize:84\n",
      "shuffle DB :test, b:100, 5-way, 5-shot, 15-query, resize:84\n",
      "step: 0 \ttraining acc: [0.2        0.32       0.33       0.41       0.44333333 0.41666667]\n",
      "Test acc: [0.2087 0.2695 0.3003 0.3167 0.332  0.3484]\n",
      "step: 100 \ttraining acc: [0.19333333 0.42       0.42333333 0.46666667 0.46666667 0.46333333]\n",
      "step: 200 \ttraining acc: [0.20666667 0.44333333 0.43333333 0.47       0.53666667 0.53333333]\n",
      "step: 300 \ttraining acc: [0.21       0.40666667 0.41333333 0.44       0.44666667 0.46      ]\n",
      "step: 400 \ttraining acc: [0.18       0.52666667 0.50666667 0.52       0.51666667 0.51666667]\n",
      "step: 500 \ttraining acc: [0.22666667 0.44       0.46333333 0.47       0.49333333 0.49666667]\n",
      "step: 600 \ttraining acc: [0.18       0.44666667 0.50333333 0.49       0.49       0.49333333]\n",
      "step: 700 \ttraining acc: [0.15       0.51666667 0.55666667 0.57       0.58333333 0.59      ]\n",
      "step: 800 \ttraining acc: [0.24333333 0.47333333 0.53       0.51666667 0.51       0.5       ]\n",
      "step: 900 \ttraining acc: [0.17       0.46333333 0.49333333 0.49       0.50333333 0.49333333]\n",
      "step: 1000 \ttraining acc: [0.22333333 0.57       0.57       0.6        0.58666667 0.59666667]\n",
      "Test acc: [0.2039 0.5024 0.522  0.5303 0.531  0.5317]\n",
      "step: 1100 \ttraining acc: [0.17333333 0.56666667 0.54333333 0.56666667 0.55       0.56      ]\n",
      "step: 1200 \ttraining acc: [0.18666667 0.54       0.53333333 0.56333333 0.54666667 0.57666667]\n",
      "step: 1300 \ttraining acc: [0.22       0.56333333 0.58333333 0.59       0.58666667 0.59      ]\n",
      "step: 1400 \ttraining acc: [0.24666667 0.62       0.62666667 0.59666667 0.61333333 0.60333333]\n",
      "step: 1500 \ttraining acc: [0.18333333 0.6        0.64333333 0.64666667 0.63       0.63333333]\n",
      "step: 1600 \ttraining acc: [0.19666667 0.54333333 0.57666667 0.56333333 0.57666667 0.58      ]\n",
      "step: 1700 \ttraining acc: [0.19666667 0.64333333 0.64333333 0.66666667 0.65333333 0.64333333]\n",
      "step: 1800 \ttraining acc: [0.21666667 0.53666667 0.54666667 0.57       0.56333333 0.58333333]\n",
      "step: 1900 \ttraining acc: [0.2        0.61       0.63333333 0.66       0.65       0.65666667]\n",
      "step: 2000 \ttraining acc: [0.21666667 0.70666667 0.74666667 0.75333333 0.75666667 0.76666667]\n",
      "Test acc: [0.1968 0.5356 0.559  0.564  0.569  0.568 ]\n",
      "step: 2100 \ttraining acc: [0.23       0.57       0.56666667 0.59       0.58       0.59      ]\n",
      "step: 2200 \ttraining acc: [0.15       0.55       0.56666667 0.54666667 0.56666667 0.57333333]\n",
      "step: 2300 \ttraining acc: [0.26666667 0.69666667 0.67666667 0.69       0.68333333 0.68      ]\n",
      "step: 2400 \ttraining acc: [0.11666667 0.52333333 0.56666667 0.56       0.56333333 0.56333333]\n",
      "step: 0 \ttraining acc: [0.16333333 0.58666667 0.57       0.59333333 0.57666667 0.58333333]\n",
      "Test acc: [0.213  0.562  0.575  0.5796 0.578  0.578 ]\n",
      "step: 100 \ttraining acc: [0.22666667 0.66       0.67       0.68333333 0.68666667 0.68333333]\n",
      "step: 200 \ttraining acc: [0.22666667 0.63       0.63       0.64666667 0.66       0.65333333]\n",
      "step: 300 \ttraining acc: [0.21333333 0.66333333 0.69666667 0.69666667 0.69666667 0.7       ]\n",
      "step: 400 \ttraining acc: [0.14       0.65666667 0.67333333 0.67333333 0.68666667 0.68333333]\n",
      "step: 500 \ttraining acc: [0.21666667 0.65666667 0.67666667 0.67       0.66666667 0.67      ]\n",
      "step: 600 \ttraining acc: [0.12666667 0.60666667 0.65666667 0.65666667 0.66666667 0.67333333]\n",
      "step: 700 \ttraining acc: [0.21333333 0.55333333 0.57333333 0.57666667 0.58       0.58      ]\n",
      "step: 800 \ttraining acc: [0.19       0.70333333 0.73       0.74333333 0.73       0.74      ]\n",
      "step: 900 \ttraining acc: [0.20333333 0.59       0.62333333 0.61333333 0.61       0.62      ]\n",
      "step: 1000 \ttraining acc: [0.13333333 0.64666667 0.67       0.69333333 0.68       0.69      ]\n",
      "Test acc: [0.2164 0.572  0.5835 0.5938 0.597  0.5977]\n",
      "step: 1100 \ttraining acc: [0.17333333 0.55333333 0.56333333 0.55333333 0.55333333 0.55333333]\n",
      "step: 1200 \ttraining acc: [0.24666667 0.59333333 0.63       0.62       0.62666667 0.62666667]\n",
      "step: 1300 \ttraining acc: [0.19       0.66666667 0.69       0.69333333 0.70333333 0.69666667]\n",
      "step: 1400 \ttraining acc: [0.2        0.64       0.61333333 0.62666667 0.64       0.64      ]\n",
      "step: 1500 \ttraining acc: [0.19333333 0.6        0.63666667 0.61666667 0.64666667 0.63666667]\n",
      "step: 1600 \ttraining acc: [0.20333333 0.60666667 0.63666667 0.67666667 0.68333333 0.68333333]\n",
      "step: 1700 \ttraining acc: [0.2        0.49666667 0.58333333 0.62666667 0.62666667 0.62      ]\n",
      "step: 1800 \ttraining acc: [0.2        0.61666667 0.65       0.68       0.68333333 0.68333333]\n",
      "step: 1900 \ttraining acc: [0.2        0.58666667 0.63       0.65333333 0.64666667 0.65      ]\n",
      "step: 2000 \ttraining acc: [0.2        0.56333333 0.61666667 0.62       0.62333333 0.63      ]\n",
      "Test acc: [0.2    0.526  0.5845 0.594  0.5967 0.5986]\n",
      "step: 2100 \ttraining acc: [0.2        0.61333333 0.69       0.68333333 0.69333333 0.69666667]\n",
      "step: 2200 \ttraining acc: [0.2        0.65333333 0.7        0.71666667 0.70666667 0.70333333]\n",
      "step: 2300 \ttraining acc: [0.2        0.65333333 0.69       0.69333333 0.70333333 0.7       ]\n",
      "step: 2400 \ttraining acc: [0.2        0.54333333 0.60666667 0.60333333 0.6        0.60333333]\n",
      "step: 0 \ttraining acc: [0.2        0.62666667 0.63333333 0.66       0.66333333 0.66333333]\n",
      "Test acc: [0.2    0.5425 0.587  0.6016 0.6055 0.607 ]\n",
      "step: 100 \ttraining acc: [0.2        0.53333333 0.56       0.57666667 0.59       0.58666667]\n",
      "step: 200 \ttraining acc: [0.2        0.66       0.64333333 0.68       0.68       0.68      ]\n",
      "step: 300 \ttraining acc: [0.2        0.59       0.6        0.63333333 0.63333333 0.62333333]\n",
      "step: 400 \ttraining acc: [0.21       0.64333333 0.69666667 0.68333333 0.69       0.68666667]\n",
      "step: 500 \ttraining acc: [0.2        0.63333333 0.7        0.69333333 0.69333333 0.69333333]\n",
      "step: 600 \ttraining acc: [0.2        0.68333333 0.70333333 0.71333333 0.71333333 0.70666667]\n",
      "step: 700 \ttraining acc: [0.18666667 0.57       0.63666667 0.65666667 0.65666667 0.65666667]\n",
      "step: 800 \ttraining acc: [0.2        0.61       0.63       0.63666667 0.63666667 0.63666667]\n",
      "step: 900 \ttraining acc: [0.2        0.7        0.72666667 0.70666667 0.70666667 0.70333333]\n",
      "step: 1000 \ttraining acc: [0.19       0.66333333 0.69666667 0.70333333 0.70333333 0.69666667]\n",
      "Test acc: [0.2003 0.5474 0.591  0.6016 0.606  0.606 ]\n",
      "step: 1100 \ttraining acc: [0.19       0.6        0.63       0.62666667 0.63333333 0.64      ]\n",
      "step: 1200 \ttraining acc: [0.20666667 0.61333333 0.68       0.67333333 0.68333333 0.68      ]\n",
      "step: 1300 \ttraining acc: [0.20333333 0.57666667 0.59333333 0.60333333 0.60666667 0.60666667]\n",
      "step: 1400 \ttraining acc: [0.20333333 0.63       0.69666667 0.71333333 0.71       0.71666667]\n",
      "step: 1500 \ttraining acc: [0.20666667 0.74666667 0.78       0.79       0.79       0.78666667]\n",
      "step: 1600 \ttraining acc: [0.20333333 0.62666667 0.67333333 0.69666667 0.69333333 0.69333333]\n",
      "step: 1700 \ttraining acc: [0.21       0.67333333 0.72666667 0.73333333 0.73       0.73333333]\n",
      "step: 1800 \ttraining acc: [0.19666667 0.69666667 0.69       0.69666667 0.7        0.7       ]\n",
      "step: 1900 \ttraining acc: [0.18666667 0.65666667 0.69       0.69       0.68       0.69      ]\n",
      "step: 2000 \ttraining acc: [0.2        0.68       0.70333333 0.71       0.71333333 0.71333333]\n",
      "Test acc: [0.1976 0.5684 0.6035 0.6104 0.6123 0.6147]\n",
      "step: 2100 \ttraining acc: [0.2        0.54333333 0.58666667 0.57666667 0.58       0.58      ]\n",
      "step: 2200 \ttraining acc: [0.20666667 0.67       0.70666667 0.72       0.72       0.72      ]\n",
      "step: 2300 \ttraining acc: [0.19333333 0.61666667 0.65       0.67       0.66666667 0.66666667]\n",
      "step: 2400 \ttraining acc: [0.20666667 0.69333333 0.72666667 0.72333333 0.72       0.71      ]\n",
      "step: 0 \ttraining acc: [0.19       0.66333333 0.66666667 0.68       0.67666667 0.68333333]\n",
      "Test acc: [0.1959 0.5825 0.6094 0.6206 0.6226 0.6226]\n",
      "step: 100 \ttraining acc: [0.2        0.63333333 0.64666667 0.64666667 0.64333333 0.65333333]\n",
      "step: 200 \ttraining acc: [0.19333333 0.74333333 0.77666667 0.77333333 0.77666667 0.77666667]\n",
      "step: 300 \ttraining acc: [0.22333333 0.74       0.76333333 0.76333333 0.76333333 0.75666667]\n",
      "step: 400 \ttraining acc: [0.21666667 0.60666667 0.67       0.69333333 0.7        0.70333333]\n",
      "step: 500 \ttraining acc: [0.18666667 0.65       0.66666667 0.65333333 0.65333333 0.65666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 600 \ttraining acc: [0.19666667 0.71       0.73666667 0.73333333 0.74666667 0.74666667]\n",
      "step: 700 \ttraining acc: [0.27333333 0.63       0.70333333 0.70666667 0.70333333 0.71      ]\n",
      "step: 800 \ttraining acc: [0.18333333 0.62333333 0.67       0.67666667 0.67666667 0.67666667]\n",
      "step: 900 \ttraining acc: [0.20666667 0.60666667 0.67333333 0.66666667 0.65333333 0.65333333]\n",
      "step: 1000 \ttraining acc: [0.24333333 0.66666667 0.75333333 0.76       0.75666667 0.75333333]\n",
      "Test acc: [0.195  0.5776 0.6035 0.611  0.6143 0.6167]\n",
      "step: 1100 \ttraining acc: [0.18666667 0.75666667 0.78333333 0.79       0.78666667 0.79      ]\n",
      "step: 1200 \ttraining acc: [0.19333333 0.65333333 0.72       0.74333333 0.75       0.75      ]\n",
      "step: 1300 \ttraining acc: [0.19       0.65       0.62333333 0.62333333 0.62666667 0.62666667]\n",
      "step: 1400 \ttraining acc: [0.18       0.69333333 0.66333333 0.66       0.66333333 0.66      ]\n",
      "step: 1500 \ttraining acc: [0.19333333 0.59       0.60666667 0.62       0.62       0.62333333]\n",
      "step: 1600 \ttraining acc: [0.20666667 0.61666667 0.62       0.63       0.63666667 0.63666667]\n",
      "step: 1700 \ttraining acc: [0.23333333 0.65333333 0.70333333 0.71       0.72333333 0.72333333]\n",
      "step: 1800 \ttraining acc: [0.22       0.69333333 0.72       0.72       0.72666667 0.73      ]\n",
      "step: 1900 \ttraining acc: [0.21       0.66       0.69333333 0.69666667 0.7        0.7       ]\n",
      "step: 2000 \ttraining acc: [0.21333333 0.63333333 0.65666667 0.65666667 0.66666667 0.67      ]\n",
      "Test acc: [0.1985 0.5796 0.611  0.614  0.616  0.6177]\n",
      "step: 2100 \ttraining acc: [0.20333333 0.68666667 0.70333333 0.7        0.70333333 0.70666667]\n",
      "step: 2200 \ttraining acc: [0.21333333 0.71       0.73       0.73666667 0.73666667 0.73333333]\n",
      "step: 2300 \ttraining acc: [0.22333333 0.57       0.60333333 0.59666667 0.61       0.61333333]\n",
      "step: 2400 \ttraining acc: [0.21       0.61333333 0.66333333 0.68       0.68333333 0.68333333]\n",
      "step: 0 \ttraining acc: [0.21       0.63333333 0.63333333 0.66       0.66333333 0.66      ]\n",
      "Test acc: [0.1986 0.584  0.6094 0.616  0.618  0.6187]\n",
      "step: 100 \ttraining acc: [0.19666667 0.67666667 0.71333333 0.70333333 0.70666667 0.70666667]\n",
      "step: 200 \ttraining acc: [0.20333333 0.64666667 0.71666667 0.71666667 0.72       0.72333333]\n",
      "step: 300 \ttraining acc: [0.18666667 0.65333333 0.68666667 0.70666667 0.71666667 0.71666667]\n",
      "step: 400 \ttraining acc: [0.22666667 0.64666667 0.68       0.68       0.68666667 0.69      ]\n",
      "step: 500 \ttraining acc: [0.23333333 0.66666667 0.71       0.71666667 0.72       0.72      ]\n",
      "step: 600 \ttraining acc: [0.23666667 0.74       0.74666667 0.76       0.75666667 0.75333333]\n",
      "step: 700 \ttraining acc: [0.22666667 0.51333333 0.55       0.55333333 0.56       0.56      ]\n",
      "step: 800 \ttraining acc: [0.16666667 0.6        0.64666667 0.67333333 0.67666667 0.67666667]\n",
      "step: 900 \ttraining acc: [0.16666667 0.64666667 0.67666667 0.67       0.66666667 0.66333333]\n",
      "step: 1000 \ttraining acc: [0.23       0.63       0.66666667 0.67333333 0.68333333 0.68666667]\n",
      "Test acc: [0.209  0.5713 0.6064 0.6104 0.611  0.6113]\n",
      "step: 1100 \ttraining acc: [0.22666667 0.61333333 0.69333333 0.7        0.7        0.70333333]\n",
      "step: 1200 \ttraining acc: [0.25333333 0.61333333 0.65333333 0.66       0.66333333 0.66333333]\n",
      "step: 1300 \ttraining acc: [0.19333333 0.7        0.75333333 0.75       0.75666667 0.76      ]\n",
      "step: 1400 \ttraining acc: [0.19333333 0.69       0.69       0.69333333 0.7        0.69333333]\n",
      "step: 1500 \ttraining acc: [0.2        0.63666667 0.67666667 0.68666667 0.69666667 0.70666667]\n",
      "step: 1600 \ttraining acc: [0.18       0.65       0.68       0.69333333 0.70666667 0.70666667]\n",
      "step: 1700 \ttraining acc: [0.18333333 0.69666667 0.74333333 0.73       0.73       0.73      ]\n",
      "step: 1800 \ttraining acc: [0.22333333 0.66333333 0.74666667 0.75333333 0.76       0.76      ]\n",
      "step: 1900 \ttraining acc: [0.18333333 0.67666667 0.71666667 0.71666667 0.71666667 0.71666667]\n",
      "step: 2000 \ttraining acc: [0.24333333 0.7        0.71666667 0.72666667 0.73       0.73333333]\n",
      "Test acc: [0.2057 0.5815 0.6094 0.6113 0.614  0.615 ]\n",
      "step: 2100 \ttraining acc: [0.20666667 0.69666667 0.75666667 0.76666667 0.77       0.77      ]\n",
      "step: 2200 \ttraining acc: [0.18333333 0.62       0.62       0.62666667 0.62333333 0.62      ]\n",
      "step: 2300 \ttraining acc: [0.21666667 0.71333333 0.75333333 0.76333333 0.77       0.77333333]\n",
      "step: 2400 \ttraining acc: [0.26333333 0.70333333 0.7        0.7        0.69666667 0.70666667]\n",
      "step: 0 \ttraining acc: [0.19       0.63666667 0.67333333 0.67333333 0.68       0.68      ]\n",
      "Test acc: [0.1931 0.571  0.608  0.6123 0.613  0.6143]\n",
      "step: 100 \ttraining acc: [0.22       0.55333333 0.64333333 0.64666667 0.65       0.65666667]\n",
      "step: 200 \ttraining acc: [0.18666667 0.71       0.74333333 0.74666667 0.74666667 0.74333333]\n",
      "step: 300 \ttraining acc: [0.19333333 0.67       0.73       0.72       0.72333333 0.72      ]\n",
      "step: 400 \ttraining acc: [0.22       0.64666667 0.68333333 0.69333333 0.70333333 0.7       ]\n",
      "step: 500 \ttraining acc: [0.24333333 0.65333333 0.68333333 0.70333333 0.71333333 0.71      ]\n",
      "step: 600 \ttraining acc: [0.23333333 0.68       0.71       0.70666667 0.71333333 0.71666667]\n",
      "step: 700 \ttraining acc: [0.22       0.65666667 0.72666667 0.73       0.72666667 0.72      ]\n",
      "step: 800 \ttraining acc: [0.19333333 0.71666667 0.74666667 0.75       0.74333333 0.75      ]\n",
      "step: 900 \ttraining acc: [0.19       0.70333333 0.76333333 0.77       0.77       0.78      ]\n",
      "step: 1000 \ttraining acc: [0.19       0.64333333 0.67       0.68666667 0.69666667 0.69      ]\n",
      "Test acc: [0.2001 0.5786 0.602  0.611  0.6123 0.6123]\n",
      "step: 1100 \ttraining acc: [0.13666667 0.68       0.7        0.71       0.71333333 0.71666667]\n",
      "step: 1200 \ttraining acc: [0.21666667 0.75       0.78       0.78666667 0.78333333 0.78333333]\n",
      "step: 1300 \ttraining acc: [0.2        0.68       0.71666667 0.70666667 0.71333333 0.71333333]\n",
      "step: 1400 \ttraining acc: [0.16666667 0.68       0.7        0.71333333 0.71       0.70333333]\n",
      "step: 1500 \ttraining acc: [0.19333333 0.76333333 0.75333333 0.75       0.75       0.75      ]\n",
      "step: 1600 \ttraining acc: [0.19333333 0.71333333 0.73333333 0.71666667 0.71666667 0.71666667]\n",
      "step: 1700 \ttraining acc: [0.21333333 0.60666667 0.61666667 0.62       0.62333333 0.62333333]\n",
      "step: 1800 \ttraining acc: [0.21333333 0.69333333 0.77333333 0.78       0.78       0.78333333]\n",
      "step: 1900 \ttraining acc: [0.20666667 0.68       0.70666667 0.71666667 0.72       0.72      ]\n",
      "step: 2000 \ttraining acc: [0.18666667 0.77666667 0.78333333 0.79       0.79       0.79      ]\n",
      "Test acc: [0.1986 0.5874 0.614  0.616  0.619  0.6196]\n",
      "step: 2100 \ttraining acc: [0.11       0.76       0.75666667 0.77       0.77       0.77      ]\n",
      "step: 2200 \ttraining acc: [0.23       0.63333333 0.68333333 0.69333333 0.68666667 0.69      ]\n",
      "step: 2300 \ttraining acc: [0.20666667 0.63666667 0.71333333 0.71333333 0.71666667 0.72333333]\n",
      "step: 2400 \ttraining acc: [0.28666667 0.65       0.72333333 0.72666667 0.72       0.72333333]\n",
      "step: 0 \ttraining acc: [0.16       0.71333333 0.71666667 0.71333333 0.71666667 0.72      ]\n",
      "Test acc: [0.2003 0.573  0.602  0.6074 0.6104 0.6104]\n",
      "step: 100 \ttraining acc: [0.17666667 0.72       0.74       0.74       0.73666667 0.73666667]\n",
      "step: 200 \ttraining acc: [0.19666667 0.61333333 0.66666667 0.66666667 0.67666667 0.67666667]\n",
      "step: 300 \ttraining acc: [0.22       0.66333333 0.72666667 0.72666667 0.72666667 0.73      ]\n",
      "step: 400 \ttraining acc: [0.24666667 0.50666667 0.57666667 0.57666667 0.57666667 0.57666667]\n",
      "step: 500 \ttraining acc: [0.18666667 0.77333333 0.81666667 0.82333333 0.82666667 0.83333333]\n",
      "step: 600 \ttraining acc: [0.16333333 0.67333333 0.69666667 0.70333333 0.70333333 0.70333333]\n",
      "step: 700 \ttraining acc: [0.20333333 0.73333333 0.76333333 0.76333333 0.76333333 0.76666667]\n",
      "step: 800 \ttraining acc: [0.22333333 0.72       0.72666667 0.74666667 0.75       0.75      ]\n",
      "step: 900 \ttraining acc: [0.21666667 0.67       0.68666667 0.7        0.71       0.70666667]\n",
      "step: 1000 \ttraining acc: [0.21666667 0.62       0.71333333 0.7        0.71       0.70666667]\n",
      "Test acc: [0.1989 0.5767 0.6055 0.609  0.6084 0.611 ]\n",
      "step: 1100 \ttraining acc: [0.20333333 0.64666667 0.7        0.7        0.7        0.69666667]\n",
      "step: 1200 \ttraining acc: [0.12333333 0.72       0.74333333 0.74666667 0.75333333 0.75666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1300 \ttraining acc: [0.22       0.81333333 0.82       0.82333333 0.82333333 0.82333333]\n",
      "step: 1400 \ttraining acc: [0.18666667 0.70333333 0.75333333 0.76       0.76       0.75666667]\n",
      "step: 1500 \ttraining acc: [0.24333333 0.72666667 0.77666667 0.77333333 0.77333333 0.77666667]\n",
      "step: 1600 \ttraining acc: [0.22       0.72333333 0.75       0.74666667 0.74666667 0.75666667]\n",
      "step: 1700 \ttraining acc: [0.20333333 0.69666667 0.74       0.74666667 0.74666667 0.75      ]\n",
      "step: 1800 \ttraining acc: [0.19       0.70666667 0.71666667 0.72       0.72666667 0.73      ]\n",
      "step: 1900 \ttraining acc: [0.21       0.66666667 0.73666667 0.74       0.73       0.73333333]\n",
      "step: 2000 \ttraining acc: [0.21666667 0.67333333 0.70666667 0.70666667 0.70666667 0.71666667]\n",
      "Test acc: [0.2126 0.576  0.6    0.602  0.6055 0.6074]\n",
      "step: 2100 \ttraining acc: [0.19666667 0.69666667 0.71333333 0.73666667 0.74666667 0.75333333]\n",
      "step: 2200 \ttraining acc: [0.22666667 0.62       0.65       0.65333333 0.65666667 0.65333333]\n",
      "step: 2300 \ttraining acc: [0.25666667 0.70333333 0.76333333 0.77666667 0.78       0.77666667]\n",
      "step: 2400 \ttraining acc: [0.25333333 0.66666667 0.68666667 0.68       0.67666667 0.67666667]\n",
      "step: 0 \ttraining acc: [0.19666667 0.66333333 0.73       0.72333333 0.71666667 0.71333333]\n",
      "Test acc: [0.2037 0.5845 0.6133 0.616  0.6177 0.6187]\n",
      "step: 100 \ttraining acc: [0.20666667 0.6        0.62666667 0.64       0.64666667 0.64666667]\n",
      "step: 200 \ttraining acc: [0.19       0.74666667 0.76666667 0.76       0.76666667 0.76333333]\n",
      "step: 300 \ttraining acc: [0.24333333 0.66       0.71666667 0.71666667 0.71666667 0.72      ]\n",
      "step: 400 \ttraining acc: [0.19666667 0.64666667 0.67       0.67666667 0.68       0.67666667]\n",
      "step: 500 \ttraining acc: [0.20666667 0.65333333 0.71       0.72       0.72333333 0.73333333]\n",
      "step: 600 \ttraining acc: [0.22       0.62       0.64666667 0.65666667 0.65666667 0.66666667]\n",
      "step: 700 \ttraining acc: [0.22333333 0.72       0.75666667 0.75333333 0.74666667 0.75666667]\n",
      "step: 800 \ttraining acc: [0.19333333 0.74       0.74       0.75       0.76       0.76      ]\n",
      "step: 900 \ttraining acc: [0.17666667 0.70333333 0.75333333 0.75       0.74333333 0.74666667]\n",
      "step: 1000 \ttraining acc: [0.15       0.73333333 0.80666667 0.79666667 0.79666667 0.79333333]\n",
      "Test acc: [0.1968 0.584  0.6113 0.6147 0.6147 0.616 ]\n",
      "step: 1100 \ttraining acc: [0.22333333 0.67333333 0.74       0.73666667 0.73666667 0.73333333]\n",
      "step: 1200 \ttraining acc: [0.19666667 0.62333333 0.68       0.68       0.67666667 0.66666667]\n",
      "step: 1300 \ttraining acc: [0.22666667 0.67666667 0.74       0.74       0.74666667 0.74666667]\n",
      "step: 1400 \ttraining acc: [0.23666667 0.72       0.74       0.74       0.73666667 0.73666667]\n",
      "step: 1500 \ttraining acc: [0.16333333 0.66333333 0.73333333 0.73666667 0.73       0.73666667]\n",
      "step: 1600 \ttraining acc: [0.25666667 0.69333333 0.72666667 0.73666667 0.74       0.74      ]\n",
      "step: 1700 \ttraining acc: [0.25666667 0.68666667 0.69       0.71333333 0.71666667 0.72333333]\n",
      "step: 1800 \ttraining acc: [0.17333333 0.73333333 0.73       0.73333333 0.74       0.73666667]\n",
      "step: 1900 \ttraining acc: [0.22666667 0.74666667 0.81       0.8        0.80333333 0.80666667]\n",
      "step: 2000 \ttraining acc: [0.18       0.64333333 0.67333333 0.67666667 0.68       0.68666667]\n",
      "Test acc: [0.1957 0.5767 0.5957 0.599  0.6006 0.6025]\n",
      "step: 2100 \ttraining acc: [0.23333333 0.68       0.72       0.72       0.72666667 0.72666667]\n",
      "step: 2200 \ttraining acc: [0.23       0.67       0.69666667 0.67666667 0.68333333 0.68      ]\n",
      "step: 2300 \ttraining acc: [0.21666667 0.73666667 0.77333333 0.78333333 0.78666667 0.78666667]\n",
      "step: 2400 \ttraining acc: [0.22666667 0.54666667 0.63333333 0.63       0.63       0.63333333]\n",
      "step: 0 \ttraining acc: [0.18333333 0.7        0.77333333 0.76666667 0.77       0.77      ]\n",
      "Test acc: [0.1997 0.5815 0.613  0.613  0.613  0.614 ]\n",
      "step: 100 \ttraining acc: [0.21666667 0.72333333 0.72333333 0.73333333 0.73666667 0.73666667]\n",
      "step: 200 \ttraining acc: [0.19333333 0.65666667 0.70333333 0.71       0.71       0.71333333]\n",
      "step: 300 \ttraining acc: [0.19666667 0.71666667 0.71       0.71666667 0.71333333 0.71333333]\n",
      "step: 400 \ttraining acc: [0.19333333 0.72       0.77666667 0.77666667 0.78       0.78      ]\n",
      "step: 500 \ttraining acc: [0.17333333 0.77666667 0.78333333 0.78666667 0.79       0.79333333]\n",
      "step: 600 \ttraining acc: [0.17       0.64333333 0.66333333 0.66333333 0.66666667 0.67      ]\n",
      "step: 700 \ttraining acc: [0.16       0.74666667 0.74333333 0.74333333 0.74333333 0.74333333]\n",
      "step: 800 \ttraining acc: [0.22       0.74       0.76666667 0.77       0.77       0.77      ]\n",
      "step: 900 \ttraining acc: [0.21       0.70666667 0.68333333 0.69666667 0.69333333 0.70333333]\n",
      "step: 1000 \ttraining acc: [0.16       0.73333333 0.77       0.77       0.77       0.77333333]\n",
      "Test acc: [0.2001 0.5786 0.6055 0.61   0.611  0.6113]\n",
      "step: 1100 \ttraining acc: [0.13333333 0.67       0.68666667 0.68666667 0.68333333 0.68333333]\n",
      "step: 1200 \ttraining acc: [0.2        0.67666667 0.70666667 0.71       0.71       0.71666667]\n",
      "step: 1300 \ttraining acc: [0.18       0.62666667 0.69333333 0.7        0.7        0.70333333]\n",
      "step: 1400 \ttraining acc: [0.17666667 0.72       0.75       0.75       0.75       0.75      ]\n",
      "step: 1500 \ttraining acc: [0.14333333 0.64333333 0.68666667 0.67666667 0.68       0.68333333]\n",
      "step: 1600 \ttraining acc: [0.22       0.70333333 0.74       0.75333333 0.75666667 0.75      ]\n",
      "step: 1700 \ttraining acc: [0.17333333 0.73333333 0.74666667 0.75333333 0.75333333 0.75333333]\n",
      "step: 1800 \ttraining acc: [0.20666667 0.65666667 0.73333333 0.73666667 0.73       0.73      ]\n",
      "step: 1900 \ttraining acc: [0.17333333 0.62666667 0.66666667 0.66333333 0.66666667 0.66666667]\n",
      "step: 2000 \ttraining acc: [0.15       0.74666667 0.76333333 0.76666667 0.77       0.77      ]\n",
      "Test acc: [0.2028 0.5776 0.6094 0.611  0.612  0.613 ]\n",
      "step: 2100 \ttraining acc: [0.23333333 0.72333333 0.76       0.76666667 0.79       0.79      ]\n",
      "step: 2200 \ttraining acc: [0.17333333 0.73333333 0.76333333 0.76       0.75666667 0.75333333]\n",
      "step: 2300 \ttraining acc: [0.2        0.69666667 0.73       0.73333333 0.73333333 0.73333333]\n",
      "step: 2400 \ttraining acc: [0.24333333 0.69666667 0.73666667 0.75333333 0.74666667 0.75666667]\n",
      "step: 0 \ttraining acc: [0.21       0.72       0.71333333 0.72666667 0.72333333 0.71666667]\n",
      "Test acc: [0.1969 0.592  0.6104 0.6113 0.613  0.6157]\n",
      "step: 100 \ttraining acc: [0.24333333 0.71666667 0.78       0.76666667 0.76666667 0.76666667]\n",
      "step: 200 \ttraining acc: [0.21333333 0.74333333 0.76       0.76333333 0.75666667 0.76      ]\n",
      "step: 300 \ttraining acc: [0.18666667 0.77333333 0.78666667 0.79333333 0.8        0.80333333]\n",
      "step: 400 \ttraining acc: [0.15       0.70666667 0.73       0.73       0.73333333 0.73333333]\n",
      "step: 500 \ttraining acc: [0.21       0.68333333 0.71       0.72       0.73       0.72666667]\n",
      "step: 600 \ttraining acc: [0.21666667 0.67333333 0.77333333 0.77666667 0.78       0.78      ]\n",
      "step: 700 \ttraining acc: [0.22666667 0.7        0.75666667 0.76       0.75333333 0.75333333]\n",
      "step: 800 \ttraining acc: [0.17       0.79333333 0.79666667 0.79333333 0.79333333 0.79      ]\n",
      "step: 900 \ttraining acc: [0.28333333 0.74333333 0.73666667 0.75       0.75333333 0.76      ]\n",
      "step: 1000 \ttraining acc: [0.17666667 0.73333333 0.72666667 0.73333333 0.73333333 0.73666667]\n",
      "Test acc: [0.1992 0.5845 0.604  0.609  0.61   0.611 ]\n",
      "step: 1100 \ttraining acc: [0.18333333 0.7        0.74       0.74333333 0.74666667 0.75333333]\n",
      "step: 1200 \ttraining acc: [0.21666667 0.70666667 0.71666667 0.73       0.72666667 0.73      ]\n",
      "step: 1300 \ttraining acc: [0.19666667 0.78333333 0.81       0.83       0.83       0.83      ]\n",
      "step: 1400 \ttraining acc: [0.12666667 0.63666667 0.65333333 0.65       0.65       0.65333333]\n",
      "step: 1500 \ttraining acc: [0.23333333 0.68       0.74333333 0.75666667 0.75333333 0.75333333]\n",
      "step: 1600 \ttraining acc: [0.15       0.72666667 0.73333333 0.73333333 0.73       0.72666667]\n",
      "step: 1700 \ttraining acc: [0.19666667 0.69666667 0.75333333 0.76333333 0.77333333 0.77      ]\n",
      "step: 1800 \ttraining acc: [0.19333333 0.83666667 0.84666667 0.84666667 0.85       0.85333333]\n",
      "step: 1900 \ttraining acc: [0.23       0.67       0.71       0.71       0.71       0.71333333]\n",
      "step: 2000 \ttraining acc: [0.21       0.77       0.79333333 0.80666667 0.81666667 0.81      ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: [0.2013 0.5806 0.6074 0.6104 0.61   0.6104]\n",
      "step: 2100 \ttraining acc: [0.18       0.71       0.75       0.74666667 0.74333333 0.73666667]\n",
      "step: 2200 \ttraining acc: [0.19       0.71666667 0.79333333 0.79333333 0.79333333 0.79333333]\n",
      "step: 2300 \ttraining acc: [0.13333333 0.72333333 0.74333333 0.74666667 0.75333333 0.75333333]\n",
      "step: 2400 \ttraining acc: [0.19333333 0.63333333 0.67333333 0.68333333 0.68666667 0.68666667]\n",
      "step: 0 \ttraining acc: [0.18       0.72       0.77333333 0.77666667 0.77333333 0.77666667]\n",
      "Test acc: [0.1942 0.5845 0.607  0.609  0.6084 0.6094]\n",
      "step: 100 \ttraining acc: [0.20333333 0.67666667 0.66666667 0.67       0.67       0.66333333]\n",
      "step: 200 \ttraining acc: [0.18666667 0.70333333 0.72666667 0.72333333 0.72       0.72      ]\n",
      "step: 300 \ttraining acc: [0.16666667 0.68666667 0.69666667 0.70333333 0.70333333 0.70333333]\n",
      "step: 400 \ttraining acc: [0.23666667 0.76666667 0.77666667 0.77666667 0.77333333 0.77666667]\n",
      "step: 500 \ttraining acc: [0.18666667 0.77333333 0.8        0.80333333 0.80333333 0.81      ]\n",
      "step: 600 \ttraining acc: [0.22666667 0.77       0.82       0.83       0.83       0.83333333]\n",
      "step: 700 \ttraining acc: [0.16333333 0.64666667 0.65333333 0.66666667 0.67333333 0.67333333]\n",
      "step: 800 \ttraining acc: [0.23       0.71333333 0.75       0.75       0.75333333 0.76      ]\n",
      "step: 900 \ttraining acc: [0.21666667 0.72       0.74       0.74       0.74666667 0.75666667]\n",
      "step: 1000 \ttraining acc: [0.24666667 0.71666667 0.74333333 0.75333333 0.75666667 0.76      ]\n",
      "Test acc: [0.1998 0.5845 0.6113 0.613  0.615  0.616 ]\n",
      "step: 1100 \ttraining acc: [0.15666667 0.78       0.78       0.78666667 0.79       0.79      ]\n",
      "step: 1200 \ttraining acc: [0.21666667 0.75666667 0.80666667 0.81       0.81666667 0.81333333]\n",
      "step: 1300 \ttraining acc: [0.21       0.75666667 0.77333333 0.78       0.78       0.78      ]\n",
      "step: 1400 \ttraining acc: [0.20666667 0.65333333 0.69666667 0.69666667 0.7        0.69666667]\n",
      "step: 1500 \ttraining acc: [0.23333333 0.64333333 0.68666667 0.69       0.69333333 0.69666667]\n",
      "step: 1600 \ttraining acc: [0.22333333 0.66666667 0.70333333 0.70666667 0.70666667 0.71      ]\n",
      "step: 1700 \ttraining acc: [0.18666667 0.76333333 0.81666667 0.82333333 0.82333333 0.82333333]\n",
      "step: 1800 \ttraining acc: [0.19666667 0.66666667 0.72666667 0.72666667 0.73333333 0.73      ]\n",
      "step: 1900 \ttraining acc: [0.22333333 0.71       0.75       0.75666667 0.75666667 0.75666667]\n",
      "step: 2000 \ttraining acc: [0.14333333 0.69666667 0.71666667 0.72333333 0.72666667 0.72666667]\n",
      "Test acc: [0.1993 0.5884 0.6157 0.6177 0.619  0.6206]\n",
      "step: 2100 \ttraining acc: [0.21       0.70333333 0.70666667 0.71333333 0.70666667 0.70666667]\n",
      "step: 2200 \ttraining acc: [0.15666667 0.71       0.74       0.74       0.73666667 0.73      ]\n",
      "step: 2300 \ttraining acc: [0.25333333 0.69333333 0.73       0.73666667 0.73666667 0.73666667]\n",
      "step: 2400 \ttraining acc: [0.22333333 0.65333333 0.66666667 0.66333333 0.67       0.67      ]\n",
      "step: 0 \ttraining acc: [0.19       0.68333333 0.72333333 0.72       0.72       0.70333333]\n",
      "Test acc: [0.1934 0.5845 0.6084 0.6094 0.6104 0.61  ]\n",
      "step: 100 \ttraining acc: [0.19333333 0.7        0.73666667 0.74666667 0.74666667 0.74666667]\n",
      "step: 200 \ttraining acc: [0.19       0.67333333 0.70333333 0.71       0.71       0.72      ]\n",
      "step: 300 \ttraining acc: [0.17       0.65       0.67666667 0.67333333 0.68       0.68      ]\n",
      "step: 400 \ttraining acc: [0.2        0.70666667 0.74333333 0.74666667 0.75       0.74666667]\n",
      "step: 500 \ttraining acc: [0.20666667 0.67333333 0.70333333 0.71       0.71333333 0.71333333]\n",
      "step: 600 \ttraining acc: [0.19333333 0.65333333 0.67       0.67       0.67       0.66666667]\n",
      "step: 700 \ttraining acc: [0.22666667 0.68333333 0.71666667 0.72666667 0.72666667 0.73666667]\n",
      "step: 800 \ttraining acc: [0.23666667 0.63       0.63       0.63666667 0.63666667 0.64333333]\n",
      "step: 900 \ttraining acc: [0.13666667 0.72333333 0.74       0.74       0.74666667 0.73666667]\n",
      "step: 1000 \ttraining acc: [0.15666667 0.75       0.79333333 0.79       0.79333333 0.79666667]\n",
      "Test acc: [0.2004 0.581  0.608  0.6113 0.613  0.614 ]\n",
      "step: 1100 \ttraining acc: [0.2        0.68333333 0.74       0.74666667 0.74333333 0.74333333]\n",
      "step: 1200 \ttraining acc: [0.15333333 0.67666667 0.72       0.72333333 0.72333333 0.72666667]\n",
      "step: 1300 \ttraining acc: [0.20333333 0.69333333 0.75333333 0.76       0.76       0.76      ]\n",
      "step: 1400 \ttraining acc: [0.22666667 0.7        0.72666667 0.72333333 0.73333333 0.73333333]\n",
      "step: 1500 \ttraining acc: [0.23       0.72666667 0.76       0.76666667 0.77333333 0.77333333]\n",
      "step: 1600 \ttraining acc: [0.19       0.71       0.74333333 0.74666667 0.74666667 0.74666667]\n",
      "step: 1700 \ttraining acc: [0.17666667 0.65666667 0.66666667 0.68       0.68       0.68333333]\n",
      "step: 1800 \ttraining acc: [0.25333333 0.65666667 0.70666667 0.70333333 0.70333333 0.70333333]\n",
      "step: 1900 \ttraining acc: [0.18333333 0.76666667 0.78       0.78       0.78333333 0.78666667]\n",
      "step: 2000 \ttraining acc: [0.22333333 0.74       0.75       0.75       0.74666667 0.75      ]\n",
      "Test acc: [0.2051 0.577  0.6006 0.6035 0.6045 0.605 ]\n",
      "step: 2100 \ttraining acc: [0.2        0.73333333 0.76666667 0.77666667 0.77666667 0.77666667]\n",
      "step: 2200 \ttraining acc: [0.21666667 0.76       0.75666667 0.76666667 0.76333333 0.76666667]\n",
      "step: 2300 \ttraining acc: [0.17       0.61       0.63666667 0.62333333 0.62333333 0.62      ]\n",
      "step: 2400 \ttraining acc: [0.25666667 0.71333333 0.75333333 0.75333333 0.75333333 0.75666667]\n",
      "step: 0 \ttraining acc: [0.18       0.66333333 0.68       0.67666667 0.67333333 0.67333333]\n",
      "Test acc: [0.2037 0.589  0.6074 0.611  0.61   0.6104]\n",
      "step: 100 \ttraining acc: [0.22333333 0.7        0.72333333 0.73       0.72666667 0.72      ]\n",
      "step: 200 \ttraining acc: [0.20666667 0.75       0.76666667 0.77       0.77       0.76666667]\n",
      "step: 300 \ttraining acc: [0.20666667 0.74333333 0.76333333 0.77333333 0.78333333 0.78666667]\n",
      "step: 400 \ttraining acc: [0.13       0.72666667 0.73666667 0.74       0.73666667 0.74      ]\n",
      "step: 500 \ttraining acc: [0.16       0.71333333 0.73666667 0.73333333 0.72666667 0.72333333]\n",
      "step: 600 \ttraining acc: [0.21       0.66666667 0.69666667 0.69333333 0.69       0.69333333]\n",
      "step: 700 \ttraining acc: [0.18666667 0.72333333 0.75333333 0.75333333 0.76       0.76      ]\n",
      "step: 800 \ttraining acc: [0.22       0.68666667 0.75       0.75666667 0.75666667 0.75333333]\n",
      "step: 900 \ttraining acc: [0.16666667 0.83       0.83       0.82333333 0.82666667 0.82666667]\n",
      "step: 1000 \ttraining acc: [0.22666667 0.72666667 0.75333333 0.76333333 0.76333333 0.76333333]\n",
      "Test acc: [0.2028 0.5977 0.616  0.62   0.6206 0.6196]\n",
      "step: 1100 \ttraining acc: [0.21333333 0.72       0.71333333 0.72       0.72       0.72333333]\n",
      "step: 1200 \ttraining acc: [0.12       0.76333333 0.8        0.79666667 0.79666667 0.8       ]\n",
      "step: 1300 \ttraining acc: [0.18666667 0.72666667 0.75333333 0.74333333 0.74333333 0.74333333]\n",
      "step: 1400 \ttraining acc: [0.2        0.65333333 0.66666667 0.66       0.66       0.67      ]\n",
      "step: 1500 \ttraining acc: [0.18333333 0.65       0.68       0.69666667 0.69666667 0.7       ]\n",
      "step: 1600 \ttraining acc: [0.20333333 0.70333333 0.78       0.77666667 0.77666667 0.77666667]\n",
      "step: 1700 \ttraining acc: [0.22       0.72333333 0.74333333 0.75       0.74666667 0.74333333]\n",
      "step: 1800 \ttraining acc: [0.19       0.68666667 0.71333333 0.72333333 0.72666667 0.72666667]\n",
      "step: 1900 \ttraining acc: [0.23       0.71       0.74333333 0.75333333 0.75333333 0.76      ]\n",
      "step: 2000 \ttraining acc: [0.21333333 0.77       0.77       0.76666667 0.77333333 0.77666667]\n",
      "Test acc: [0.1981 0.6    0.6143 0.6147 0.6167 0.6177]\n",
      "step: 2100 \ttraining acc: [0.16666667 0.74666667 0.76666667 0.77333333 0.77333333 0.77666667]\n",
      "step: 2200 \ttraining acc: [0.21333333 0.75333333 0.78333333 0.78       0.77666667 0.77666667]\n",
      "step: 2300 \ttraining acc: [0.21333333 0.76       0.75       0.75       0.74666667 0.74666667]\n",
      "step: 2400 \ttraining acc: [0.22666667 0.73666667 0.78333333 0.78666667 0.78666667 0.78666667]\n",
      "step: 0 \ttraining acc: [0.20333333 0.76333333 0.78       0.78       0.78333333 0.78333333]\n",
      "Test acc: [0.1963 0.5913 0.604  0.608  0.608  0.611 ]\n",
      "step: 100 \ttraining acc: [0.2        0.75333333 0.82666667 0.83666667 0.83666667 0.83666667]\n",
      "step: 200 \ttraining acc: [0.26666667 0.7        0.70333333 0.71       0.71       0.71      ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 300 \ttraining acc: [0.12333333 0.73333333 0.75666667 0.75333333 0.75       0.74666667]\n",
      "step: 400 \ttraining acc: [0.18333333 0.69666667 0.72       0.72666667 0.72333333 0.72333333]\n",
      "step: 500 \ttraining acc: [0.28       0.73       0.75666667 0.76666667 0.76666667 0.76666667]\n",
      "step: 600 \ttraining acc: [0.21333333 0.71       0.76666667 0.76       0.76333333 0.76333333]\n",
      "step: 700 \ttraining acc: [0.15333333 0.83333333 0.84333333 0.84333333 0.84333333 0.84333333]\n",
      "step: 800 \ttraining acc: [0.26       0.81333333 0.78666667 0.79       0.79       0.8       ]\n",
      "step: 900 \ttraining acc: [0.2        0.65       0.67666667 0.68666667 0.69       0.69333333]\n",
      "step: 1000 \ttraining acc: [0.22       0.72333333 0.77       0.76666667 0.77       0.76666667]\n",
      "Test acc: [0.1963 0.5835 0.6    0.603  0.605  0.6064]\n",
      "step: 1100 \ttraining acc: [0.14333333 0.74666667 0.76       0.76       0.76666667 0.76666667]\n",
      "step: 1200 \ttraining acc: [0.16666667 0.80333333 0.80666667 0.80333333 0.81       0.80666667]\n",
      "step: 1300 \ttraining acc: [0.17666667 0.62666667 0.58       0.58       0.59       0.59      ]\n",
      "step: 1400 \ttraining acc: [0.29333333 0.73       0.73       0.74333333 0.74333333 0.74      ]\n",
      "step: 1500 \ttraining acc: [0.19333333 0.78       0.78666667 0.79       0.78666667 0.79      ]\n",
      "step: 1600 \ttraining acc: [0.18       0.61333333 0.65666667 0.66666667 0.66333333 0.66666667]\n",
      "step: 1700 \ttraining acc: [0.17       0.73       0.75333333 0.75       0.75333333 0.76      ]\n",
      "step: 1800 \ttraining acc: [0.21666667 0.7        0.75333333 0.75666667 0.75       0.75333333]\n",
      "step: 1900 \ttraining acc: [0.25       0.64666667 0.68666667 0.69       0.69       0.69666667]\n",
      "step: 2000 \ttraining acc: [0.23333333 0.78333333 0.79666667 0.79666667 0.8        0.8       ]\n",
      "Test acc: [0.2008 0.586  0.614  0.615  0.616  0.6177]\n",
      "step: 2100 \ttraining acc: [0.17666667 0.68333333 0.73333333 0.73       0.74       0.74      ]\n",
      "step: 2200 \ttraining acc: [0.16       0.78666667 0.78666667 0.78666667 0.78666667 0.78333333]\n",
      "step: 2300 \ttraining acc: [0.22       0.72       0.75333333 0.75666667 0.75666667 0.76      ]\n",
      "step: 2400 \ttraining acc: [0.22333333 0.65       0.68666667 0.68666667 0.68666667 0.69      ]\n",
      "step: 0 \ttraining acc: [0.18333333 0.69333333 0.73       0.72666667 0.72666667 0.72666667]\n",
      "Test acc: [0.197  0.589  0.6035 0.605  0.6064 0.6064]\n",
      "step: 100 \ttraining acc: [0.20333333 0.78333333 0.83333333 0.82666667 0.82333333 0.82333333]\n",
      "step: 200 \ttraining acc: [0.14       0.75333333 0.78       0.79       0.79333333 0.79333333]\n",
      "step: 300 \ttraining acc: [0.13333333 0.85333333 0.86666667 0.86333333 0.86666667 0.87      ]\n",
      "step: 400 \ttraining acc: [0.19666667 0.71666667 0.74666667 0.73666667 0.73666667 0.74      ]\n",
      "step: 500 \ttraining acc: [0.19333333 0.73       0.75666667 0.75       0.74666667 0.75      ]\n",
      "step: 600 \ttraining acc: [0.11333333 0.75333333 0.74333333 0.75       0.75333333 0.75333333]\n",
      "step: 700 \ttraining acc: [0.27333333 0.73666667 0.74333333 0.74333333 0.74666667 0.75      ]\n",
      "step: 800 \ttraining acc: [0.17333333 0.67666667 0.67333333 0.67333333 0.67       0.68      ]\n",
      "step: 900 \ttraining acc: [0.16       0.67       0.66666667 0.66333333 0.65666667 0.65666667]\n",
      "step: 1000 \ttraining acc: [0.21666667 0.69666667 0.71       0.71666667 0.72       0.71666667]\n",
      "Test acc: [0.2009 0.5884 0.608  0.6084 0.6094 0.611 ]\n",
      "step: 1100 \ttraining acc: [0.20666667 0.71       0.72333333 0.72333333 0.72       0.72333333]\n",
      "step: 1200 \ttraining acc: [0.22       0.68666667 0.7        0.68666667 0.68666667 0.68666667]\n",
      "step: 1300 \ttraining acc: [0.21666667 0.73666667 0.72666667 0.71666667 0.71666667 0.71333333]\n",
      "step: 1400 \ttraining acc: [0.21333333 0.7        0.72666667 0.73       0.73666667 0.74333333]\n",
      "step: 1500 \ttraining acc: [0.19333333 0.67666667 0.70666667 0.70666667 0.71       0.71333333]\n",
      "step: 1600 \ttraining acc: [0.18       0.74333333 0.77666667 0.77666667 0.78       0.78      ]\n",
      "step: 1700 \ttraining acc: [0.20666667 0.76       0.75666667 0.75666667 0.75666667 0.75666667]\n",
      "step: 1800 \ttraining acc: [0.21666667 0.64666667 0.69666667 0.7        0.7        0.70333333]\n",
      "step: 1900 \ttraining acc: [0.19       0.72       0.73666667 0.73333333 0.73333333 0.73666667]\n",
      "step: 2000 \ttraining acc: [0.2  0.75 0.75 0.76 0.76 0.76]\n",
      "Test acc: [0.2025 0.5835 0.609  0.611  0.611  0.6123]\n",
      "step: 2100 \ttraining acc: [0.24333333 0.76       0.77       0.76666667 0.76666667 0.76666667]\n",
      "step: 2200 \ttraining acc: [0.2        0.77666667 0.79       0.78333333 0.78       0.78333333]\n",
      "step: 2300 \ttraining acc: [0.18       0.71666667 0.69666667 0.69666667 0.69666667 0.69333333]\n",
      "step: 2400 \ttraining acc: [0.18       0.75666667 0.78333333 0.78333333 0.78333333 0.78666667]\n",
      "step: 0 \ttraining acc: [0.22666667 0.81666667 0.82333333 0.82333333 0.82333333 0.83333333]\n",
      "Test acc: [0.1945 0.58   0.6094 0.609  0.6104 0.612 ]\n",
      "step: 100 \ttraining acc: [0.20666667 0.76333333 0.82       0.83333333 0.83666667 0.84333333]\n",
      "step: 200 \ttraining acc: [0.09666667 0.71666667 0.72333333 0.72333333 0.72333333 0.72333333]\n",
      "step: 300 \ttraining acc: [0.22333333 0.78666667 0.81333333 0.81333333 0.81666667 0.81666667]\n",
      "step: 400 \ttraining acc: [0.20333333 0.75333333 0.75666667 0.76       0.76666667 0.77      ]\n",
      "step: 500 \ttraining acc: [0.21333333 0.67666667 0.69666667 0.7        0.7        0.69666667]\n",
      "step: 600 \ttraining acc: [0.23333333 0.72       0.76666667 0.77666667 0.77666667 0.77666667]\n",
      "step: 700 \ttraining acc: [0.17       0.72333333 0.74666667 0.74333333 0.74666667 0.75      ]\n",
      "step: 800 \ttraining acc: [0.18       0.75       0.78666667 0.79666667 0.79666667 0.79666667]\n",
      "step: 900 \ttraining acc: [0.19       0.68666667 0.74       0.74666667 0.74666667 0.74666667]\n",
      "step: 1000 \ttraining acc: [0.16666667 0.75       0.77       0.77333333 0.77       0.77333333]\n",
      "Test acc: [0.2035 0.582  0.6006 0.603  0.603  0.605 ]\n",
      "step: 1100 \ttraining acc: [0.20666667 0.74666667 0.76333333 0.76333333 0.76666667 0.76666667]\n",
      "step: 1200 \ttraining acc: [0.23666667 0.73333333 0.77       0.77333333 0.77666667 0.78      ]\n",
      "step: 1300 \ttraining acc: [0.19333333 0.72       0.76       0.75666667 0.75666667 0.75666667]\n",
      "step: 1400 \ttraining acc: [0.22333333 0.72       0.73       0.73       0.73333333 0.73666667]\n",
      "step: 1500 \ttraining acc: [0.18333333 0.67       0.69666667 0.7        0.70333333 0.70333333]\n",
      "step: 1600 \ttraining acc: [0.20666667 0.71       0.72333333 0.72       0.72       0.71666667]\n",
      "step: 1700 \ttraining acc: [0.19333333 0.82666667 0.82       0.82333333 0.82333333 0.82333333]\n",
      "step: 1800 \ttraining acc: [0.22       0.76666667 0.76666667 0.78       0.78666667 0.78333333]\n",
      "step: 1900 \ttraining acc: [0.19333333 0.77333333 0.78       0.78666667 0.78666667 0.79      ]\n",
      "step: 2000 \ttraining acc: [0.22333333 0.68666667 0.74666667 0.74       0.74333333 0.74333333]\n",
      "Test acc: [0.1897 0.597  0.616  0.6177 0.6177 0.618 ]\n",
      "step: 2100 \ttraining acc: [0.17333333 0.78333333 0.81333333 0.81333333 0.81333333 0.81333333]\n",
      "step: 2200 \ttraining acc: [0.17333333 0.72333333 0.73333333 0.73666667 0.74       0.74333333]\n",
      "step: 2300 \ttraining acc: [0.25333333 0.73       0.78       0.79       0.79333333 0.79333333]\n",
      "step: 2400 \ttraining acc: [0.19       0.75       0.8        0.8        0.80333333 0.80333333]\n",
      "step: 0 \ttraining acc: [0.21333333 0.75666667 0.79333333 0.79       0.79       0.79333333]\n",
      "Test acc: [0.1942 0.582  0.603  0.605  0.605  0.6045]\n",
      "step: 100 \ttraining acc: [0.14       0.72666667 0.74666667 0.75666667 0.75666667 0.75333333]\n",
      "step: 200 \ttraining acc: [0.23666667 0.64666667 0.67       0.66666667 0.66333333 0.66666667]\n",
      "step: 300 \ttraining acc: [0.21333333 0.68333333 0.69666667 0.7        0.70333333 0.70666667]\n",
      "step: 400 \ttraining acc: [0.20666667 0.83       0.83       0.83333333 0.83333333 0.83666667]\n",
      "step: 500 \ttraining acc: [0.17666667 0.76666667 0.77333333 0.77       0.77       0.76666667]\n",
      "step: 600 \ttraining acc: [0.24333333 0.71       0.73       0.72666667 0.73       0.73333333]\n",
      "step: 700 \ttraining acc: [0.23333333 0.76333333 0.77666667 0.77666667 0.77666667 0.77666667]\n",
      "step: 800 \ttraining acc: [0.2        0.73       0.73666667 0.74666667 0.75       0.75333333]\n",
      "step: 900 \ttraining acc: [0.22       0.81666667 0.81666667 0.81       0.81       0.81333333]\n",
      "step: 1000 \ttraining acc: [0.17666667 0.70333333 0.7        0.69666667 0.7        0.71333333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: [0.1982 0.5786 0.6016 0.603  0.603  0.605 ]\n",
      "step: 1100 \ttraining acc: [0.24333333 0.69333333 0.72333333 0.72666667 0.73333333 0.73666667]\n",
      "step: 1200 \ttraining acc: [0.17333333 0.76666667 0.78333333 0.78666667 0.79       0.8       ]\n",
      "step: 1300 \ttraining acc: [0.16333333 0.65666667 0.68666667 0.68666667 0.69       0.69      ]\n",
      "step: 1400 \ttraining acc: [0.27333333 0.73333333 0.77333333 0.77333333 0.77666667 0.77333333]\n",
      "step: 1500 \ttraining acc: [0.21333333 0.79       0.82333333 0.83       0.84       0.84      ]\n",
      "step: 1600 \ttraining acc: [0.17333333 0.73       0.75333333 0.75333333 0.76333333 0.76333333]\n",
      "step: 1700 \ttraining acc: [0.2        0.68       0.72       0.72333333 0.72333333 0.72      ]\n",
      "step: 1800 \ttraining acc: [0.17333333 0.72666667 0.73       0.73333333 0.73333333 0.73666667]\n",
      "step: 1900 \ttraining acc: [0.19666667 0.74666667 0.74333333 0.74       0.73666667 0.74      ]\n",
      "step: 2000 \ttraining acc: [0.18666667 0.73666667 0.76333333 0.76333333 0.75333333 0.75666667]\n",
      "Test acc: [0.207  0.5835 0.605  0.607  0.6074 0.6074]\n",
      "step: 2100 \ttraining acc: [0.15666667 0.70333333 0.69666667 0.69666667 0.7        0.69666667]\n",
      "step: 2200 \ttraining acc: [0.13333333 0.69333333 0.69333333 0.7        0.69666667 0.71      ]\n",
      "step: 2300 \ttraining acc: [0.18       0.76666667 0.78333333 0.78333333 0.78333333 0.78      ]\n",
      "step: 2400 \ttraining acc: [0.19333333 0.71666667 0.74666667 0.74666667 0.74666667 0.75333333]\n",
      "step: 0 \ttraining acc: [0.16333333 0.73       0.77666667 0.78       0.78       0.77333333]\n",
      "Test acc: [0.1976 0.576  0.609  0.612  0.6143 0.616 ]\n",
      "step: 100 \ttraining acc: [0.18333333 0.74666667 0.77       0.78333333 0.78666667 0.79      ]\n",
      "step: 200 \ttraining acc: [0.21       0.65       0.68333333 0.69333333 0.7        0.69      ]\n",
      "step: 300 \ttraining acc: [0.19666667 0.66333333 0.68666667 0.68333333 0.68333333 0.68333333]\n",
      "step: 400 \ttraining acc: [0.18333333 0.81       0.81333333 0.81333333 0.81333333 0.81      ]\n",
      "step: 500 \ttraining acc: [0.18666667 0.77333333 0.81666667 0.82333333 0.82333333 0.83      ]\n",
      "step: 600 \ttraining acc: [0.20666667 0.72333333 0.79       0.78666667 0.79       0.79333333]\n",
      "step: 700 \ttraining acc: [0.17       0.65       0.65666667 0.64666667 0.65       0.65333333]\n",
      "step: 800 \ttraining acc: [0.16333333 0.72333333 0.71       0.71666667 0.72333333 0.73666667]\n",
      "step: 900 \ttraining acc: [0.19666667 0.70333333 0.70333333 0.71       0.71       0.71333333]\n",
      "step: 1000 \ttraining acc: [0.24       0.72333333 0.74666667 0.74666667 0.74666667 0.74333333]\n",
      "Test acc: [0.2068 0.5923 0.6084 0.6113 0.612  0.6123]\n",
      "step: 1100 \ttraining acc: [0.23333333 0.67333333 0.73333333 0.73666667 0.73666667 0.73333333]\n",
      "step: 1200 \ttraining acc: [0.31666667 0.75333333 0.76       0.75       0.75333333 0.75333333]\n",
      "step: 1300 \ttraining acc: [0.18666667 0.80333333 0.80333333 0.80333333 0.80333333 0.8       ]\n",
      "step: 1400 \ttraining acc: [0.15333333 0.73666667 0.76333333 0.76333333 0.76666667 0.76666667]\n",
      "step: 1500 \ttraining acc: [0.20666667 0.79       0.78333333 0.77666667 0.77333333 0.77      ]\n",
      "step: 1600 \ttraining acc: [0.16       0.76333333 0.79       0.79       0.79333333 0.79      ]\n",
      "step: 1700 \ttraining acc: [0.16       0.68333333 0.70666667 0.70666667 0.72       0.72333333]\n",
      "step: 1800 \ttraining acc: [0.19       0.76       0.76       0.76666667 0.76666667 0.76666667]\n",
      "step: 1900 \ttraining acc: [0.21       0.76333333 0.79666667 0.8        0.80666667 0.80666667]\n",
      "step: 2000 \ttraining acc: [0.2        0.75       0.76666667 0.76666667 0.76666667 0.76666667]\n",
      "Test acc: [0.1862 0.5884 0.612  0.6123 0.614  0.614 ]\n",
      "step: 2100 \ttraining acc: [0.28333333 0.73333333 0.73666667 0.73666667 0.74       0.74      ]\n",
      "step: 2200 \ttraining acc: [0.23333333 0.72       0.79333333 0.79666667 0.79333333 0.79333333]\n",
      "step: 2300 \ttraining acc: [0.12       0.79333333 0.8        0.8        0.79666667 0.79666667]\n",
      "step: 2400 \ttraining acc: [0.18       0.78666667 0.80666667 0.80333333 0.80333333 0.80333333]\n",
      "step: 0 \ttraining acc: [0.22666667 0.77666667 0.78333333 0.79       0.79333333 0.79333333]\n",
      "Test acc: [0.2039 0.589  0.608  0.6094 0.611  0.6113]\n",
      "step: 100 \ttraining acc: [0.2        0.77666667 0.79       0.79666667 0.8        0.8       ]\n",
      "step: 200 \ttraining acc: [0.21333333 0.76       0.80666667 0.81       0.81       0.80666667]\n",
      "step: 300 \ttraining acc: [0.23666667 0.67333333 0.74       0.74333333 0.74666667 0.75333333]\n",
      "step: 400 \ttraining acc: [0.2        0.77333333 0.75333333 0.76       0.77       0.77333333]\n",
      "step: 500 \ttraining acc: [0.22333333 0.69666667 0.69       0.69       0.68666667 0.68333333]\n",
      "step: 600 \ttraining acc: [0.13666667 0.66666667 0.71333333 0.71666667 0.71333333 0.71333333]\n",
      "step: 700 \ttraining acc: [0.17       0.79333333 0.78333333 0.79333333 0.79333333 0.79333333]\n",
      "step: 800 \ttraining acc: [0.22666667 0.74666667 0.75666667 0.75666667 0.75666667 0.75333333]\n",
      "step: 900 \ttraining acc: [0.25333333 0.75666667 0.78333333 0.78666667 0.78666667 0.78666667]\n",
      "step: 1000 \ttraining acc: [0.17       0.73333333 0.74       0.74333333 0.74       0.74666667]\n",
      "Test acc: [0.1963 0.586  0.604  0.6084 0.609  0.611 ]\n",
      "step: 1100 \ttraining acc: [0.24666667 0.76666667 0.79333333 0.78666667 0.78333333 0.79      ]\n",
      "step: 1200 \ttraining acc: [0.26333333 0.77       0.82666667 0.82333333 0.82666667 0.82666667]\n",
      "step: 1300 \ttraining acc: [0.18666667 0.72       0.75666667 0.76333333 0.76       0.76      ]\n",
      "step: 1400 \ttraining acc: [0.18333333 0.63       0.66666667 0.67333333 0.67666667 0.68      ]\n",
      "step: 1500 \ttraining acc: [0.16666667 0.71333333 0.73333333 0.74       0.73666667 0.74      ]\n",
      "step: 1600 \ttraining acc: [0.2        0.74333333 0.78       0.78666667 0.79333333 0.79333333]\n",
      "step: 1700 \ttraining acc: [0.22       0.74333333 0.77       0.78       0.78       0.77333333]\n",
      "step: 1800 \ttraining acc: [0.21       0.73       0.75       0.75       0.75333333 0.75333333]\n",
      "step: 1900 \ttraining acc: [0.21666667 0.73333333 0.75333333 0.75666667 0.76       0.75666667]\n",
      "step: 2000 \ttraining acc: [0.13666667 0.74666667 0.75333333 0.76       0.75666667 0.76      ]\n",
      "Test acc: [0.2019 0.579  0.603  0.6055 0.606  0.6064]\n",
      "step: 2100 \ttraining acc: [0.25333333 0.68666667 0.73       0.73666667 0.73666667 0.73666667]\n",
      "step: 2200 \ttraining acc: [0.08666667 0.69333333 0.69666667 0.69666667 0.70333333 0.71      ]\n",
      "step: 2300 \ttraining acc: [0.21       0.71666667 0.73       0.73       0.73       0.73      ]\n",
      "step: 2400 \ttraining acc: [0.22       0.75       0.77666667 0.78       0.78       0.78      ]\n",
      "step: 0 \ttraining acc: [0.19       0.75333333 0.77666667 0.78333333 0.78666667 0.78666667]\n",
      "Test acc: [0.2028 0.5864 0.604  0.6064 0.6084 0.609 ]\n",
      "step: 100 \ttraining acc: [0.15       0.76333333 0.79       0.79       0.79       0.79      ]\n",
      "step: 200 \ttraining acc: [0.19666667 0.7        0.73666667 0.74       0.74333333 0.75      ]\n",
      "step: 300 \ttraining acc: [0.19333333 0.71666667 0.74666667 0.75       0.75       0.74666667]\n",
      "step: 400 \ttraining acc: [0.18       0.76333333 0.78       0.78666667 0.79       0.79      ]\n",
      "step: 500 \ttraining acc: [0.17       0.71       0.75666667 0.75666667 0.75666667 0.75      ]\n",
      "step: 600 \ttraining acc: [0.25333333 0.73       0.76       0.76666667 0.77666667 0.77666667]\n",
      "step: 700 \ttraining acc: [0.2        0.71       0.73333333 0.73333333 0.73666667 0.74333333]\n",
      "step: 800 \ttraining acc: [0.22666667 0.73666667 0.76333333 0.77       0.76666667 0.76666667]\n",
      "step: 900 \ttraining acc: [0.19666667 0.79       0.8        0.79333333 0.79       0.79      ]\n",
      "step: 1000 \ttraining acc: [0.19       0.74       0.77333333 0.77333333 0.77666667 0.77333333]\n",
      "Test acc: [0.1978 0.5874 0.6084 0.6084 0.6094 0.611 ]\n",
      "step: 1100 \ttraining acc: [0.17       0.71666667 0.73333333 0.73333333 0.73666667 0.74      ]\n",
      "step: 1200 \ttraining acc: [0.21333333 0.72666667 0.77666667 0.77666667 0.77666667 0.77666667]\n",
      "step: 1300 \ttraining acc: [0.17666667 0.68       0.70333333 0.71       0.71       0.71      ]\n",
      "step: 1400 \ttraining acc: [0.18666667 0.71333333 0.75       0.75       0.75       0.75333333]\n",
      "step: 1500 \ttraining acc: [0.2        0.70333333 0.71333333 0.72333333 0.72333333 0.72333333]\n",
      "step: 1600 \ttraining acc: [0.2        0.85       0.86       0.86666667 0.86333333 0.86333333]\n",
      "step: 1700 \ttraining acc: [0.20666667 0.64666667 0.70666667 0.71333333 0.71333333 0.71333333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1800 \ttraining acc: [0.22       0.7        0.74       0.74333333 0.74666667 0.75      ]\n",
      "step: 1900 \ttraining acc: [0.19       0.69       0.71333333 0.72       0.71666667 0.71666667]\n",
      "step: 2000 \ttraining acc: [0.20333333 0.74333333 0.77333333 0.78       0.78333333 0.78333333]\n",
      "Test acc: [0.1932 0.5767 0.59   0.5913 0.5923 0.594 ]\n",
      "step: 2100 \ttraining acc: [0.18       0.78333333 0.77       0.77666667 0.78       0.78333333]\n",
      "step: 2200 \ttraining acc: [0.23666667 0.67333333 0.80666667 0.81       0.81       0.81333333]\n",
      "step: 2300 \ttraining acc: [0.20666667 0.73333333 0.78666667 0.78333333 0.78666667 0.78333333]\n",
      "step: 2400 \ttraining acc: [0.13333333 0.77333333 0.8        0.80333333 0.81       0.81333333]\n",
      "step: 0 \ttraining acc: [0.25666667 0.67666667 0.70666667 0.71333333 0.71333333 0.71666667]\n",
      "Test acc: [0.1991 0.5957 0.609  0.6113 0.6113 0.612 ]\n",
      "step: 100 \ttraining acc: [0.23666667 0.74666667 0.77       0.77666667 0.78       0.77333333]\n",
      "step: 200 \ttraining acc: [0.13       0.73333333 0.74       0.74666667 0.74666667 0.74666667]\n",
      "step: 300 \ttraining acc: [0.2        0.69666667 0.75       0.75666667 0.75666667 0.74666667]\n",
      "step: 400 \ttraining acc: [0.17333333 0.71333333 0.71666667 0.71666667 0.72       0.72333333]\n",
      "step: 500 \ttraining acc: [0.20333333 0.68333333 0.71666667 0.72333333 0.72333333 0.72333333]\n",
      "step: 600 \ttraining acc: [0.16       0.74       0.78       0.78333333 0.78666667 0.78666667]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b6b1fb4fb6fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-b6b1fb4fb6fa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mx_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_spt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3.6/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "import  argparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def mean_confidence_interval(accs, confidence=0.95):\n",
    "    n = accs.shape[0]\n",
    "    m, se = np.mean(accs), scipy.stats.sem(accs)\n",
    "    h = se * scipy.stats.t._ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, h\n",
    "\n",
    "\n",
    "n_way = 5\n",
    "k_spt = 5\n",
    "epochs = 1000001\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "\n",
    "    config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [n_way, 32 * 5 * 5])\n",
    "    ]\n",
    "\n",
    "    device = torch.device('cuda:2')\n",
    "    maml = Meta(config).to(device)\n",
    "\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "#     print(maml)\n",
    "#     print('Total trainable tensors:', num)\n",
    "\n",
    "    # batchsz here means total episode number\n",
    "    mini_train = MiniImagenet('./../datasets/mini-imagenet/', mode='train', n_way=5, k_shot=5,\n",
    "                        k_query=15,\n",
    "                        batchsz=10000, resize=84)\n",
    "    mini_test = MiniImagenet('./../datasets/mini-imagenet/', mode='test', n_way=5, k_shot=5,\n",
    "                             k_query=15,\n",
    "                             batchsz=100, resize=84)\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(epochs//10000):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        db = DataLoader(mini_train, 4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db):\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "            accs = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "            \n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print('step:', step, '\\ttraining acc:', accs)\n",
    "\n",
    "            if step % 1000 == 0:  # evaluation\n",
    "                db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "\n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    accs = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs_all_test.append(accs)\n",
    "\n",
    "                # [b, update_step+1]\n",
    "                accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                print('Test acc:', accs)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-14T07:49:14.040Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML3.6",
   "language": "python",
   "name": "ml3.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
